// ============================================================================
// SERVI√áO DE INTEGRA√á√ÉO COM GLM (Zhipu AI)
// ============================================================================
// Cliente para integra√ß√£o com a API do GLM 4.5

interface GLMMessage {
  role: 'system' | 'user' | 'assistant';
  content: string;
}

interface GLMChatRequest {
  model: string;
  messages: GLMMessage[];
  temperature?: number;
  max_tokens?: number;
  top_p?: number;
  stream?: boolean;
}

interface GLMChatResponse {
  id: string;
  object: string;
  created: number;
  model: string;
  choices: {
    index: number;
    message: {
      role: string;
      content: string;
    };
    finish_reason: string;
  }[];
  usage: {
    prompt_tokens: number;
    completion_tokens: number;
    total_tokens: number;
  };
}

interface GLMEmbeddingRequest {
  model: string;
  input: string | string[];
}

interface GLMEmbeddingResponse {
  object: string;
  data: {
    object: string;
    index: number;
    embedding: number[];
  }[];
  model: string;
  usage: {
    prompt_tokens: number;
    total_tokens: number;
  };
}

export class GLMService {
  private apiKey: string;
  private baseUrl: string;
  private embeddingUrl: string;

  constructor(apiKey: string) {
    this.apiKey = apiKey;
    this.baseUrl = 'https://open.bigmodel.cn/api/paas/v4/chat/completions';
    this.embeddingUrl = 'https://open.bigmodel.cn/api/paas/v4/embeddings';
  }

  private getHeaders() {
    return {
      'Authorization': `Bearer ${this.apiKey}`,
      'Content-Type': 'application/json',
    };
  }

  /**
   * Testa a conex√£o com a API do GLM
   */
  async testConnection(): Promise<{ success: boolean; error?: string }> {
    try {
      console.log('Testando conex√£o GLM com API key:', this.apiKey.substring(0, 8) + '...');

      const response = await this.chatCompletion({
        model: 'glm-4',
        messages: [
          {
            role: 'user',
            content: 'Ol√°! Responda apenas "OK" para testar a conex√£o.'
          }
        ],
        max_tokens: 10,
        temperature: 0.1
      });

      if (response.choices && response.choices.length > 0) {
        console.log('Teste de conex√£o GLM bem-sucedido');
        return { success: true };
      } else {
        return { success: false, error: 'Resposta inv√°lida da API' };
      }
    } catch (error: any) {
      console.error('Erro no teste de conex√£o GLM:', error);

      // Tratamento de erros mais espec√≠fico
      let errorMessage = 'Erro desconhecido na conex√£o';

      if (error.message.includes('Failed to fetch') || error.name === 'TypeError') {
        errorMessage = 'Erro de rede ou CORS: N√£o foi poss√≠vel conectar ao servidor GLM. Isso pode ser devido a pol√≠ticas CORS do navegador. A API key parece v√°lida.';
      } else if (error.message.includes('401')) {
        errorMessage = 'Erro de autentica√ß√£o: Chave da API inv√°lida ou expirada.';
      } else if (error.message.includes('403')) {
        errorMessage = 'Erro de permiss√£o: Acesso negado. Verifique se a chave da API tem as permiss√µes necess√°rias.';
      } else if (error.message.includes('429')) {
        errorMessage = 'Limite de taxa excedido: Muitas requisi√ß√µes. Tente novamente em alguns minutos.';
      } else if (error.message.includes('500')) {
        errorMessage = 'Erro interno do servidor GLM. Tente novamente mais tarde.';
      } else if (error.message.includes('CORS')) {
        errorMessage = 'Erro de CORS: Problema de pol√≠tica de origem cruzada.';
      } else {
        errorMessage = error.message;
      }

      return {
        success: false,
        error: errorMessage
      };
    }
  }

  /**
   * Realiza chat completion com o GLM
   */
  async chatCompletion(request: GLMChatRequest): Promise<GLMChatResponse> {
    try {
      console.log('üöÄ [GLM Service] Sending Request:', {
        url: this.baseUrl,
        model: request.model,
        messagesCount: request.messages.length,
        temperature: request.temperature
      });

      const response = await fetch(this.baseUrl, {
        method: 'POST',
        headers: this.getHeaders(),
        body: JSON.stringify(request),
      }).catch(fetchError => {
        console.error('üí• [GLM Service] Network/Fetch Error:', fetchError);
        throw new Error(`Network Error (CORS?): ${fetchError.message}`);
      });

      if (!response.ok) {
        const errorText = await response.text();
        console.error('‚ùå [GLM Service] API Error Response:', {
          status: response.status,
          statusText: response.statusText,
          body: errorText
        });
        throw new Error(`GLM API Error: ${response.status} - ${errorText}`);
      }

      const data = await response.json();
      console.log('‚úÖ [GLM Service] Response Received:', {
        id: data.id,
        model: data.model,
        usage: data.usage
      });

      return data;
    } catch (error: any) {
      console.error('üî• [GLM Service] Method Exception:', error);
      throw error;
    }
  }

  /**
   * Gera embeddings usando GLM
   */
  async generateEmbeddings(request: GLMEmbeddingRequest): Promise<GLMEmbeddingResponse> {
    try {
      const response = await fetch(this.embeddingUrl, {
        method: 'POST',
        headers: this.getHeaders(),
        body: JSON.stringify(request),
      });

      if (!response.ok) {
        const errorText = await response.text();
        throw new Error(`GLM Embeddings API Error: ${response.status} - ${errorText}`);
      }

      return await response.json();
    } catch (error: any) {
      console.error('Erro na gera√ß√£o de embeddings GLM:', error);
      throw new Error(`Falha na gera√ß√£o de embeddings: ${error.message}`);
    }
  }

  /**
   * M√©todo espec√≠fico para an√°lise de riscos usando GLM
   */
  async analyzeRisk(riskDescription: string, context?: string): Promise<string> {
    const systemPrompt = `Voc√™ √© um especialista em an√°lise de riscos corporativos e compliance. 
Analise o risco apresentado e forne√ßa:
1. Classifica√ß√£o do risco (baixo, m√©dio, alto, cr√≠tico)
2. Impactos potenciais
3. Recomenda√ß√µes de mitiga√ß√£o
4. Controles sugeridos

Seja objetivo e t√©cnico na sua an√°lise.`;

    const userPrompt = context
      ? `Contexto: ${context}\n\nRisco a analisar: ${riskDescription}`
      : `Risco a analisar: ${riskDescription}`;

    const response = await this.chatCompletion({
      model: 'glm-4',
      messages: [
        { role: 'system', content: systemPrompt },
        { role: 'user', content: userPrompt }
      ],
      temperature: 0.3,
      max_tokens: 1000
    });

    return response.choices[0]?.message?.content || 'Erro na an√°lise do risco';
  }

  /**
   * M√©todo espec√≠fico para an√°lise de compliance usando GLM
   */
  async analyzeCompliance(requirement: string, framework: string): Promise<string> {
    const systemPrompt = `Voc√™ √© um especialista em compliance e frameworks regulat√≥rios. 
Analise o requisito apresentado no contexto do framework especificado e forne√ßa:
1. Interpreta√ß√£o do requisito
2. Evid√™ncias necess√°rias
3. Controles recomendados
4. Gaps comuns de implementa√ß√£o

Seja preciso e baseie-se nas melhores pr√°ticas do framework.`;

    const response = await this.chatCompletion({
      model: 'glm-4',
      messages: [
        { role: 'system', content: systemPrompt },
        { role: 'user', content: `Framework: ${framework}\n\nRequisito: ${requirement}` }
      ],
      temperature: 0.2,
      max_tokens: 1200
    });

    return response.choices[0]?.message?.content || 'Erro na an√°lise de compliance';
  }
}

// Factory function para criar inst√¢ncia do GLM Service
export const createGLMService = (apiKey: string): GLMService => {
  return new GLMService(apiKey);
};

// Fun√ß√£o para validar formato da API key do GLM
export const validateGLMApiKey = (apiKey: string): boolean => {
  // Formato esperado: xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx.xxxxxxxxxxxxxxxx (32.16)
  const glmKeyPattern = /^[a-f0-9]{32}\.[a-zA-Z0-9]{16}$/;
  return glmKeyPattern.test(apiKey);
};

export default GLMService;